# VLM命名改进：基于对话内容的智能命名

## 改进背景

之前的VLM命名主要依赖外貌特征，容易出现命名不准确的问题。例如：
- 不同人物穿着相似时容易混淆
- 仅凭外貌难以推测人物的真实姓名
- 无法利用对话中的人物称呼信息

## 改进内容

### 1. 增加对话上下文（核心改进）

**修改文件**: `name_speakers_with_vlm.py`

**改进点**:
为每个Speaker添加其出现时对应的对话内容：

```python
# 之前：只显示片段编号
Speaker 1:
  检测到 225 张人脸
  出现在 62 个片段: [1, 2, 5, 8, 10, ...]

# 现在：显示实际对话内容
Speaker 1:
  检测到 225 张人脸
  出现在 62 个对话片段
  相关对话内容：
    [片段1] 你好，我是阮娇
    [片段2] 裴宴，你来了
    [片段5] 小叔，这件事我会处理
    ... (最多显示15个片段)
```

### 2. 优化命名策略

**新增的prompt指导**：

```
**命名分析策略（重要！）**：

1. **对话内容优先**：
   - 仔细查看每个Speaker的"相关对话内容"
   - 如果对话中提到该人物的名字或称呼，直接使用
   - 如果对话显示该人物的职业/身份（如"老师"、"医生"），可以用作命名

2. **对话-外貌关联**：
   - 通过对话内容推测人物性别、年龄、职业
   - 结合外貌特征验证推测是否合理
   - 注意：同一个人可能穿着不同，但对话风格应该一致

3. **名字命名优先级**：
   - 第一优先：对话中直接提到的名字（如"阮娇"、"裴宴"）
   - 第二优先：对话中的称呼（如"小叔"、"张经理"）
   - 第三优先：基于对话推测的角色（如"女主角"、"男老师"）
   - **不要**仅根据外貌猜测名字

4. **关系分析**：
   - 看哪些Speaker的对话片段有重叠（同时出现）
   - 分析对话中的称呼和语气（如："您"表示尊敬）
   - 注意对话中提到的人物关系（如："你爸爸"、"我老板"）
```

### 3. 新增字段

在VLM输出的JSON中新增：

```json
"character_analysis": {
  "personality": "从对话推测的性格特点",
  "importance": "重要程度（基于出现次数和对话内容）",
  "relationship": "与其他角色的关系",
  "dialogue_characteristics": "对话特点（说话风格、常用词汇等）"  // 新增
}
```

## 技术实现

### 代码变更

**文件**: `name_speakers_with_vlm.py`

**修改部分1 - 提取对话内容**:
```python
# 为每个Speaker添加相关对话内容
speaker_info_text += f"  相关对话内容：\n"
display_segments = segments[:15]  # 最多显示15个片段，避免prompt过长
for seg_idx in display_segments:
    if 1 <= seg_idx <= len(srt_segments):
        seg_text = srt_segments[seg_idx - 1]['text']
        speaker_info_text += f"    [片段{seg_idx}] {seg_text}\n"

if len(segments) > 15:
    speaker_info_text += f"    ... 还有 {len(segments) - 15} 个片段\n"
```

**修改部分2 - 调整prompt结构**:
```python
## 2. 每个Speaker的相关对话（重要！）
以下是每个Speaker画面出现时对应的对话内容，这是判断人物身份的关键信息：

{speaker_info_text}

## 3. 完整对话内容参考（共{len(srt_segments)}个片段）
{srt_text}
```

## 预期效果

### 命名准确性提升

**场景1：对话中直接提到名字**
```
对话: "阮娇，你好"
结果: 系统能够识别出该Speaker叫"阮娇"，而不是"女主角"或"穿白衣女子"
```

**场景2：对话中的称呼**
```
对话: "小叔，这件事交给我吧"
结果: 系统识别该Speaker为"小叔"，而不是"戴眼镜男子"
```

**场景3：对话风格辅助判断**
```
Speaker A对话: "你好"、"不客气"、"谢谢"
Speaker B对话: "您好"、"麻烦您了"、"感谢您"
结果: 结合外貌，可以更准确区分两个说话人
```

### 关系分析增强

通过对话片段重叠分析：
```
Speaker 1 出现片段: [1, 2, 5, 8, 10]
Speaker 2 出现片段: [1, 2, 3, 5, 9]

重叠片段: [1, 2, 5]
-> 推断: Speaker 1 和 Speaker 2 有较多互动，可能是主要关系
```

## 使用方法

### 1. 上传视频和SRT
访问 http://localhost:5445，上传文件

### 2. 输入API Key
在"MiniMax API Key"输入框中输入你的API Key

### 3. 处理完成后查看日志

服务器日志会显示VLM prompt的详细内容：
```
调用MiniMax VLM...
  API: https://api.minimaxi.com/v1/text/chatcompletion_v2
  模型: MiniMax-Text-01
  图片数量: 10
  Prompt长度: 15000+ 字符  // 比之前更长，因为包含了对话内容
  Trace-ID: ...
```

### 4. 查看命名结果

结果中的每个人物现在会包含：
- 基于对话的准确姓名/称呼
- 对话风格分析
- 更准确的人物关系推断

## 性能考虑

### Prompt长度优化

- 每个Speaker最多显示15个对话片段
- 避免prompt过长导致API调用失败
- 15个片段通常足够VLM判断人物身份

### 计算公式

```
总Prompt长度 ≈
  基础说明 (500字) +
  完整SRT (原有) +
  每个Speaker的对话片段 (15条 × Speaker数量)
```

示例（10个Speaker）：
```
基础说明: 500字
完整SRT: 5000字
Speaker对话: 15条 × 10人 × 平均30字 = 4500字
总计: ~10000字 (在API限制范围内)
```

## 注意事项

### 1. 对话片段数量限制
当前设置为每个Speaker最多显示**15个对话片段**，可以根据需要调整：

```python
# 在 name_speakers_with_vlm.py 中修改
display_segments = segments[:15]  # 调整这个数字
```

### 2. API调用时间
由于prompt变长，API调用时间可能会增加：
- 之前: ~10秒
- 现在: ~15-20秒（取决于对话数量）

### 3. Trace-ID监控
建议关注服务器日志中的Trace-ID，用于排查VLM命名问题

## 测试建议

使用包含明显人名称呼的视频测试，例如：
- 对话中有"张三"、"李四"等明确人名
- 有"经理"、"老师"等职业称呼
- 有"爸爸"、"妈妈"等亲属称呼

对比改进前后的命名准确性。

## 当前状态

- ✅ 代码已修改
- ✅ 服务器已重启（端口5445）
- ✅ 新prompt已生效
- 📝 待测试实际效果

## 下一步

1. 使用真实视频测试命名准确性
2. 根据测试结果调整对话片段数量（15条是否合适）
3. 观察VLM的Trace-ID和返回结果
4. 持续优化prompt策略
